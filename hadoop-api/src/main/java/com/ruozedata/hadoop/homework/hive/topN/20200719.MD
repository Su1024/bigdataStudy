## 20200719作业

### 1.作业1

#### 1.1 要求
* 自己生成如下格式的数据,然后在hive中统计对应编号的*Top10*

```
统计最热门的问答对Top10
http://ruozedata.com/question/100
...
..
统计最热门的课程Top10
http://ruozedata.com/course/458655.html      <= Spark实战
http://ruozedata.com/course/458655/2.html?a=b&c=d    <= Spark Core01
                                              458655_2
```

#### 1.2 具体操作步骤

##### 1.2.1 生成对应的文件数据
参考 **GenerateLog.java** 文件

生成如下两个文件数据（考虑到git空间问题，只截取部分数据）

 * [问答对](http://git.ruozedata.com/G9/G9-04/blob/940-%E4%B8%8A%E6%B5%B7-500/20200719/data/course.log)
 * [课程](http://git.ruozedata.com/G9/G9-04/blob/940-%E4%B8%8A%E6%B5%B7-500/20200719/data/question.log)

##### 1.2.2 在hive创建对应的表 并加载数据

**建表语句**
```hiveql
create table url_log(
url string
)
```

**加载数据**
```hiveql
LOAD DATA LOCAL INPATH '/Users/sugm/work_space/G9-project/bigdataStudy/hadoop-api/data/course.log' OVERWRITE INTO TABLE url_log;

LOAD DATA LOCAL INPATH '/Users/sugm/work_space/G9-project/bigdataStudy/hadoop-api/data/question.log' OVERWRITE INTO TABLE url_log;
```

##### 1.2.3 自定义UDF 解析数据

###### 1.2.3.1 自定义udf 
我们需要将课程的url中的数字部分截取出来，详细请见 **UdfUrl.java**

###### 1.2.3.2 上传hive中
* 打包编译
* 将jar包丢到hive 中

    ```hiveql
     hive> 
         > add jar /Users/sugm/Desktop/hive-1.0-SNAPSHOT.jar;
    ```
* 注册函数（因为是开发阶段先使用临时函数的方式）

    ```hiveql
     hive> create temporary function url_parse as 'com.ruozedata.hadoop.homework.hive.topN.UdfUrl';
    ```
* 验证函数
    ```hiveql
    hive> select url_parse(url),url from url_log limit 2;
    OK
    175	http://ruozedata.com/question/175
    153	http://ruozedata.com/question/153
    Time taken: 0.039 seconds, Fetched: 2 row(s)
    hive>  
   
    ```
##### 1.2.4 求Top10
>思路:先获取每个课程id对应出现的次数，然后在根据次数排序 limit n 即可;
>具体步骤 : 1.对当前的课程id group by 求出每个课程id对应的 次数
>          2.将结果数据直接写到结果表中，表中应该是id ，count
>          3.直接对结果表做统计；

* 获取每个课程id出现的次数 并写入结果表中
```hiveql
hive> create table url_top as 
    > select url_parse(url) as url_id,count(1) url_count from url_log group by url_parse(url);
```
* 查询结果表数据
```hiveql
hive> select * from url_top;
OK
0	10944
1	11036
10	11145
100	11091
101	10945
102	11106
```

* 获取top10
```hiveql
hive> select * from url_top order by  url_count desc limit 10;
OK
35	11468
88	11378
173	11352
121	11350
24	11334
5	11331
182	11321
135	11318
114	11318
185	11316
```

### 2.作业2
#### 2.1 要求
**输入数据**

```
domain           time         traffic
gifshow.com      2020/01/01     5
yy.com           2020/01/01     4
huya.com         2020/01/01     1
gifshow.com      2020/01/20     6
gifshow.com      2020/02/01     8
yy.com           2020/01/20     5
gifshow.com      2020/02/02     7
```

**结果数据**

```
domain      月份     小计     累计
gifshow.com    2020-01    11       11
gifshow.com    2020-02    15       26
yy.com         2020-01    9         9
huya.com       2020-01    1         1
```

#### 2.2 具体步骤

#### 2.2.1 建表 导数据

```hiveql
create table domain_access(
domain string,
access_time string,
traffic int
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' 
;
```

```hiveql
LOAD DATA LOCAL INPATH '/Users/sugm/Desktop/data.txt' OVERWRITE INTO TABLE domain_access;

```

#### 2.2.2 sqlboy show time
> 思路： 若老不让用窗口，难受下
> 1.先得到每个域名 每个月的值
> 2.将每个域名 每个月的值的结果数据和自己关联系 过滤比自己小的月份，然后一个max 一个sum 搞定
>

* 获取每个月的值
```hiveql
hive> 
    > 
    > 
    > 
    > select domain,from_unixtime(unix_timestamp(access_time,'yyyy/MM/dd'),'yyyy-MM') month,sum(traffic) total_traffic from domain_access group by domain,from_unixtime(unix_timestamp(access_time,'yyyy/MM/dd'),'yyyy-MM');
OK
gifshow.com	2020-01	11
gifshow.com	2020-02	15
huya.com	2020-01	1
yy.com	2020-01	9
```
* 方便起见把每个月的值写入中间表 后面直接用 方便

```hiveql

create table domain_month as 
select domain,from_unixtime(unix_timestamp(access_time,'yyyy/MM/dd'),'yyyy-MM') month,sum(traffic) total_traffic from domain_access group by domain,from_unixtime(unix_timestamp(access_time,'yyyy/MM/dd'),'yyyy-MM');

```
* 自关联下先看看 明细数据

```hiveql

hive> select a.domain,a.month,b.domain,b.month,a.total_traffic,b.total_traffic from domain_month a left join domain_month b on a.domain = b.domain ;

OK
gifshow.com	2020-01	gifshow.com	2020-01	11	11
gifshow.com	2020-01	gifshow.com	2020-02	11	15
gifshow.com	2020-02	gifshow.com	2020-01	15	11
gifshow.com	2020-02	gifshow.com	2020-02	15	15
huya.com	2020-01	huya.com	2020-01	1	1
yy.com	2020-01	yy.com	2020-01	9	9
```
* 从上述明细数据可以看出 只需要过滤掉比自己小的月的数据既可以 最终sql 如下

```hiveql

hive> 
    > select a.domain,a.month,max(a.total_traffic),sum(b.total_traffic) from domain_month a left join domain_month b on a.domain = b.domain  where a.month>=b.month group by a.domain,a.month;
OK
gifshow.com	2020-01	11	11
gifshow.com	2020-02	15	26
huya.com	2020-01	1	1
yy.com	2020-01	9	9
```
